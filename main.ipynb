{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globalization of Science - data, calculation and preparing for web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Downloading Scopus data\n",
    "\n",
    "* Currently not working due to API request issues\n",
    "\n",
    "* Outputs into `180802_1611_AllJournals_ArReCp_2001_2017.sqlite` DB, that is available in the root directory\n",
    "\n",
    "* here to verify python code rather than real downloading, that takes two weeks to proceed\n",
    "\n",
    "* Requires API KEY with sufficient limit ( > 500 000 requests ) - special permit from Scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_API_KEY = 'e3fd43198781e92e0e07b7f543064003'\n",
    "from DownloadData import Download\n",
    "Download.downloadAll(MY_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Calculate raw globalizations\n",
    "\n",
    "from the journal level data in sqlite DB (see above) calculates Globalizations for all countries, in all years in the disciplines in particular level (4 top disciplines + All => 'TOP'; 27 narrow disciplines => 'bottom')\n",
    "\n",
    "\n",
    "* outputs into csv files specified in the `topPath` and `botPath`\n",
    "\n",
    "* The results are already available in the following files:\n",
    "\n",
    "    1. Narrow disciplines: 20181218_AllFieldsCountriesMethods_bot_all.csv\n",
    "    \n",
    "    2. Top disciplines: 20181218_AllFieldsCountriesMethods_TOP.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topPath = '20190415_AllFieldsCountriesMethods_TOP.csv'\n",
    "botPath = '20190415_AllFieldsCountriesMethods_bot.csv'\n",
    "\n",
    "from CalculateGlobalization import InternationalityCalculations as calc\n",
    "calc.CalculateEverything(topPath,'TOP')\n",
    "calc.CalculateEverything(botPath,'bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Transforms data for web\n",
    "\n",
    "Transforms raw globalization data so that it can be used in the interactive application\n",
    "\n",
    "* the data for the database are saved into the `AWS_Import` directory. These should be subsequently imported to the database\n",
    "\n",
    "* the data for dropdown lists in the application are stored in `controls_data.js` in the root directory. This file should be copied to `public/javascripts` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from previous calculation succesfully loaded ...\n",
      "Excluded all globalizations from countries and disciplines that contribute to less than 30 journals ...\n",
      "Succesfully calculated group averages ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results normalized between 0 and 1\n",
      "Data for database saved in the InteractiveWeb/DataForDB/ directory\n",
      "Data for dropdown lists in interactive webpage saved into controls_data.js\n",
      "Processing for web finished!\n"
     ]
    }
   ],
   "source": [
    "from TransformToWeb import transform\n",
    "topData = '20181218_AllFieldsCountriesMethods_TOP.xlsx' \n",
    "bottomData = '20181218_AllFieldsCountriesMethods_bot_all.csv'\n",
    "additionalData = 'populateAmazon.xlsx'\n",
    "csvDir = 'InteractiveWeb/DataForDB/'\n",
    "ddlPath = 'controls_data.js'\n",
    "\n",
    "transform.processDataForWeb(topData,bottomData,additionalData,csvDir,ddlPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Import DB and deploy server\n",
    "\n",
    "srvAdr = `ec2-18-188-88-0.us-east-2.compute.amazonaws.com`\n",
    "srvUser = `ubuntu`\n",
    "sshKey = ''\n",
    "\n",
    "dbAdr = `science-internationality-dbinstance.c3aa5fkeiz2h.us-east-2.rds.amazonaws.com:5432`\n",
    "dbUsr = `root`\n",
    "dbPass = `IDEA_Science2018`\n",
    "\n",
    "\n",
    "Postup:\n",
    "\n",
    "1) obsah slozky `InteractiveWeb` dostan na ten server\n",
    "\n",
    "2) naimportuj databazi (psql prikazy v `InteractiveWeb/DataForDB` )\n",
    "\n",
    "3) Set up node.js, install dependencies\n",
    "\n",
    "3) Spust prismu pomoci obsahu slozky v `InteractiveWeb/prisma` a `prisma.yml` a db info je predpokladam taky v `docker-compose`\n",
    "\n",
    "4) run `node bin/www > stdout.txt 2>stderr.txt  &` in root directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App description\n",
    "\n",
    "* Backend runs on AWS EC2 instance ubuntu@ec2-18-188-88-0.us-east-2.compute.amazonaws.com\n",
    "\n",
    "* Backend communicates with AWS RDS instance with PostgresSQL database science-internationality-dbinstance.c3aa5fkeiz2h.us-east-2.rds.amazonaws.com\n",
    "\n",
    "* Communication between EC2 and RDS is chanelled via two channels - `Prisma` and `pg` module in node.js - see `main/routes` directory\n",
    "\n",
    "\n",
    "### Backend\n",
    "* Main routes are decribed in the `main/routes` directory. There are only two post-requests routes\n",
    "\n",
    "1) POST route on address  `/prisma` serves the prisma route\n",
    "\n",
    "2) POST route on address `/map`  channels map data via `pg` module in node.js\n",
    "\n",
    "3) Frontend is stored in the `main/public` directory\n",
    "\n",
    "4) Node.js server is set up in the `main/bin/www` file\n",
    "\n",
    "Beckend requirements: \n",
    "* Node.js\n",
    "* npm \n",
    "* psql\n",
    "* prisma\n",
    "* docker \n",
    "* pg module\n",
    "*\n",
    "\n",
    "### Setting Postgres and Amazon RDS\n",
    "\n",
    "1) Postgres is hosted on Amazon RDS science-internationality-dbinstance.c3aa5fkeiz2h.us-east-2.rds.amazonaws.com\n",
    "\n",
    "\n",
    "## Importing a database\n",
    "\n",
    "1) Run `CalculateEverything` in the `InternationalityIndex.InternationalityCalculations.py`\n",
    "\n",
    "2) Copy the output xlsx file in the same folder as this notebook.\n",
    "\n",
    "3) Edit the first two rows in the following cell and run all cells in the notebook\n",
    "\n",
    "4) After finishing computation, copy all files from the AWS_Import directory using WinSCP\n",
    "\n",
    "    a. Connect to AWS EC2 IDEA (ubuntu@ec2-18-188-88-0.us-east-2.compute.amazonaws.com)\n",
    "    \n",
    "    b. Copy csv files from the `AWS_Import` directory to `\\home\\ubuntu\\db-admin\\csv`\n",
    "    \n",
    "5) Using Putty, run the import to Postgres\n",
    "\n",
    "    a. Connect to AWS EC2 IDEA (ubuntu@ec2-18-188-88-0.us-east-2.compute.amazonaws.com)\n",
    "    \n",
    "    b. Go to `db-admin` directory\n",
    "    \n",
    "    c. run: `psql --host=science-internationality-dbinstance.c3aa5fkeiz2h.us-east-2.rds.amazonaws.com --port=5432 --username=root --password --dbname=scienceInternationalitydb -f drop_generate_schema.sql`\n",
    "    \n",
    "    d. run: `psql --host=science-internationality-dbinstance.c3aa5fkeiz2h.us-east-2.rds.amazonaws.com --port=5432 --username=root --password --dbname=scienceInternationalitydb -f psql-import-csvs.txt`\n",
    "    \n",
    "    \n",
    "In case of problems check\n",
    "    a. Variable names - from the original excel in additionalData, through the table schema in drop_generate_schema.sql to variable names in psql-import-csvs.txt\n",
    "    \n",
    "    b. Data validity in CSVs.\n",
    "    \n",
    "    c. Also prisma query in fetcher.js should contain valid variable names! If they change, prisma should be rerun as follows:\n",
    "        1. docker-compose down\n",
    "        2. change the datamodel.yml\n",
    "        3. docker-compose up -d prisma\n",
    "        4. prisma deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
